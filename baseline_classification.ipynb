{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Download fastText for command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#! wget https://github.com/facebookresearch/fastText/archive/v0.2.0.zip\n",
    "#! unzip v0.2.0.zip\n",
    "#! cd fastText-0.2.0\n",
    "#! make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Download benchmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Download one of the benchmark dataset\n",
    "ag_df = pd.read_csv('https://github.com/mhjabreel/CharCnn_Keras/raw/master/data/ag_news_csv/train.csv',header=None\n",
    "                    ,names=['y','title','text'])\n",
    "ag_df_test = pd.read_csv('https://github.com/mhjabreel/CharCnn_Keras/raw/master/data/ag_news_csv/test.csv',header=None\n",
    "                    ,names=['y','title','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import IMDB\n",
    "##IMDB DATA\n",
    "# getting the imdb review data.\n",
    "import requests\n",
    "response = requests.get('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',stream=True)\n",
    "with open('aclImdb_v1.tar.gz','wb') as f:\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            f.write(chunk)\n",
    "            #f.flush() # forces it to write on disk \n",
    "\n",
    "\n",
    "import tarfile\n",
    "with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "import os,pandas as pd\n",
    "import tqdm\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "#pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "with tqdm.tqdm(total=50000) as t:\n",
    "    for s in ('test', 'train'):\n",
    "        for l in ('pos', 'neg'):\n",
    "            path = os.path.join(basepath, s, l)\n",
    "            for file in os.listdir(path):\n",
    "                with open(os.path.join(path, file),'r', encoding='utf-8') as infile:\n",
    "                    txt = infile.read()\n",
    "                    df = df.append([[txt, labels[l],s]],ignore_index=True)\n",
    "                    #pbar.update()\n",
    "                    t.update()\n",
    "df.columns = ['review', 'sentiment','set']\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "imdb_df = df.reindex(np.random.permutation(df.index))\n",
    "imdb_df.to_csv('imdb_sentiment.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Load IMDB: One of the benchmark from the Baselines and Bigrams\n",
    "import pandas as pd\n",
    "imdb_df = pd.read_csv('imdb_sentiment.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Movie Review example: Benchmark from Kim et al. 2014: Convolutional Neural Networks for sentence classification\n",
    "import requests\n",
    "neg = 'https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.neg'\n",
    "pos = 'https://raw.githubusercontent.com/yoonkim/CNN_sentence/master/rt-polarity.pos'\n",
    "neg = requests.get(neg).text.split('\\n')\n",
    "pos = requests.get(pos).text.split('\\n')\n",
    "docs = neg+pos\n",
    "# Make train test split\n",
    "idx = np.random.permutation(np.arange(len(tokenized)))\n",
    "\n",
    "n = int(len(idx)*0.75)\n",
    "train,test = idx[0:n],idx[n:]\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "y = np.array(([0]*len(neg))+([1]*len(pos)))\n",
    "\n",
    "df = pd.DataFrame({'text':docs,'y':y})\n",
    "train_df,test_df = df.iloc[train],df.iloc[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Format data for fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def make_fasttext_format(df,y_col,text_col,outfile,tokenizer=nltk.tokenize.TweetTokenizer().tokenize):\n",
    "    docs = df[text_col].values\n",
    "    # tokenize\n",
    "    tokenized = [' '.join(tokenizer(doc)) for doc in docs]\n",
    "    # lower\n",
    "    tokenized = [doc.lower().replace('\\n',' __newline__ ') for doc in tokenized]\n",
    "    fasttext_labels = ['__label__%s'%str(i) for i in df[y_col]]\n",
    "    fast_docs = [' '.join([fasttext_labels[i],tokenized[i]]) for i in range(len(df))]\n",
    "    with open(outfile,'w') as f:\n",
    "        f.write('\\n'.join(fast_docs))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "make_fasttext_format(ag_df,\n",
    "                     y_col='y',\n",
    "                     text_col='text'\n",
    "                     ,outfile='ag_train')\n",
    "make_fasttext_format(ag_df_test,\n",
    "                     y_col='y',\n",
    "                     text_col='text'\n",
    "                     ,outfile='ag_test')\n",
    "\n",
    "make_fasttext_format(imdb_df[imdb_df['set']=='train'],\n",
    "                     y_col='sentiment',\n",
    "                     text_col='review'\n",
    "                     ,outfile='imdb_train')\n",
    "make_fasttext_format(imdb_df[imdb_df['set']=='test'],\n",
    "                     y_col='sentiment',\n",
    "                     text_col='review'\n",
    "                     ,outfile='imdb_test')\n",
    "make_fasttext_format(train_df,\n",
    "                     y_col='y',\n",
    "                     text_col='text'\n",
    "                     ,outfile='mr_train')\n",
    "make_fasttext_format(test_df,\n",
    "                     y_col='y',\n",
    "                     text_col='text'\n",
    "                     ,outfile='mr_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run fastText classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 7M words\n",
      "Number of words:  103093\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1269315 lr:  0.000000 loss:  0.030434 ETA:   0h 0m\n",
      "N\t25000\n",
      "P@1\t0.9\n",
      "R@1\t0.9\n"
     ]
    }
   ],
   "source": [
    "# set path to fasttext\n",
    "fast_path = '/mnt/b0c8e396-e5ba-4614-be6f-146c4c861ab3/fastText-0.2.0'\n",
    "! {fast_path}/./fasttext supervised -input imdb_train -output model_fast -lr 0.5 -epoch 50 -wordNgrams 3 -dim 10 -ws 5\n",
    "#! {fast_path}/./fasttext supervised -input fasttext.train -output model_fast -lr 0.5 -epoch 200 -wordNgrams 3 -dim 100 -ws 5\n",
    "! {fast_path}/./fasttext test model_fast.bin imdb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4M words\n",
      "Number of words:  78902\n",
      "Number of labels: 4\n",
      "Progress: 100.0% words/sec/thread: 1224338 lr:  0.000000 loss:  0.031434 ETA:   0h 0m\n",
      "N\t7600\n",
      "P@1\t0.912\n",
      "R@1\t0.912\n"
     ]
    }
   ],
   "source": [
    "### Ag example\n",
    "# set path to fasttext\n",
    "fast_path = '/mnt/b0c8e396-e5ba-4614-be6f-146c4c861ab3/fastText-0.2.0'\n",
    "! {fast_path}/./fasttext supervised -input ag_train -output model_fast -lr 0.5 -epoch 50 -wordNgrams 3 -dim 10 -ws 5\n",
    "#! {fast_path}/./fasttext supervised -input fasttext.train -output model_fast -lr 0.5 -epoch 200 -wordNgrams 3 -dim 100 -ws 5\n",
    "! {fast_path}/./fasttext test model_fast.bin ag_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  17808\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  987497 lr:  0.000000 loss:  0.022001 ETA:   0h 0m\n",
      "N\t2666\n",
      "P@1\t0.761\n",
      "R@1\t0.761\n"
     ]
    }
   ],
   "source": [
    "# set path to fasttext\n",
    "fast_path = '/mnt/b0c8e396-e5ba-4614-be6f-146c4c861ab3/fastText-0.2.0'\n",
    "! {fast_path}/./fasttext supervised -input mr_train -output model_fast -lr 0.5 -epoch 300 -wordNgrams 2 -dim 100 -ws 5\n",
    "#! {fast_path}/./fasttext supervised -input fasttext.train -output model_fast -lr 0.5 -epoch 200 -wordNgrams 3 -dim 100 -ws 5\n",
    "! {fast_path}/./fasttext test model_fast.bin mr_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baselines and Bigrams implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "## Download script for searching the hyperparameters with bayesian optimization using the hyperopt package.\n",
    "import requests\n",
    "with open('get_bow_baseline.py','w') as f:\n",
    "    f.write(requests.get('https://raw.githubusercontent.com/snorreralund/test_tokenization/master/get_bow_baseline.py'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [06:07<00:00,  1.58s/it, best loss: 0.23339999999999994]\n",
      "Final accuracy and roc_auc score of tokenizer (nltk_tweet) + nb_log: 0.763 and 0.844\n"
     ]
    }
   ],
   "source": [
    "import get_bow_baseline as baseline\n",
    "baseline_mr = baseline.TokenizationTest(train_df,test_df,MAX_EVALS=150,scoring_function='accuracy_score')\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.TweetTokenizer().tokenize\n",
    "baseline_mr.evaluate('nltk_tweet',tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anything Park Chan-wook creates is guaranteed ...</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>... So some people might argue that this can't...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We are not in the fairy tale of the naked empe...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this show about 3-4 years ago. It was da...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You know Jason, you know Freddy, and you know ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  y    set\n",
       "0  Anything Park Chan-wook creates is guaranteed ...  1   test\n",
       "1  ... So some people might argue that this can't...  0   test\n",
       "2  We are not in the fairy tale of the naked empe...  0  train\n",
       "3  I saw this show about 3-4 years ago. It was da...  1  train\n",
       "4  You know Jason, you know Freddy, and you know ...  0  train"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = imdb_df.drop('Unnamed: 0',axis=1)\n",
    "imdb_df.columns = ['text','y','set']\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [3:57:23<00:00, 116.67s/it, best loss: 0.09654400000000007] \n",
      "Final accuracy and roc_auc score of tokenizer (nltk_tweet) + nb_log: 0.890 and 0.953\n"
     ]
    }
   ],
   "source": [
    "import get_bow_baseline as baseline\n",
    "baseline_imdb = baseline.TokenizationTest(imdb_df[imdb_df['set']=='train'],imdb_df[imdb_df['set']=='test']\n",
    "                                          ,MAX_EVALS=100,scoring_function='accuracy_score')\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "tokenizer = nltk.tokenize.TweetTokenizer().tokenize\n",
    "baseline_imdb.evaluate('nltk_tweet',tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (full)",
   "language": "python",
   "name": "env_full"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
